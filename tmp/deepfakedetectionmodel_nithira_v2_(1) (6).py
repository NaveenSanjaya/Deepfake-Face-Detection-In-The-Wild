# -*- coding: utf-8 -*-
"""DeepfakeDetectionModel_Nithira_v2 (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11DK6wuqOIXdavWnQRLKWu3iB8_EQ46vB
"""

import os
import random
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
from torchvision import datasets, transforms
from torchvision.models import efficientnet_b6, EfficientNet_B6_Weights
from torch.utils.data import Dataset, DataLoader, Subset
import numpy as np
from tqdm.auto import tqdm
from torch.amp import GradScaler, autocast
from sklearn.metrics import precision_recall_fscore_support, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Device configuration
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

import tarfile
from google.colab import drive
drive.mount('/content/drive')

# Define a directory to save the extracted files
output_dir = './train'
os.makedirs(output_dir, exist_ok=True)
real="/content/drive/MyDrive/Colab Notebooks/SP Cup 2025/Dataset/Tar Files/train_real.tar"
with tarfile.open(real, 'r') as tar:
    tar.extractall(path=output_dir)
    print("train real files extracted successfully!")

output_dir = './train'
fake="/content/drive/MyDrive/Colab Notebooks/SP Cup 2025/Dataset/Tar Files/train_fake.tar"
with tarfile.open(fake, 'r') as tar:
    tar.extractall(path=output_dir)
    print("train fake Files extracted successfully!")

output_dir = './valid'
real="/content/drive/MyDrive/Colab Notebooks/SP Cup 2025/Dataset/Tar Files/valid_real.tar"
with tarfile.open(real, 'r') as tar:
    tar.extractall(path=output_dir)
    print("valid real Files extracted successfully!")

output_dir = './valid'
fake="/content/drive/MyDrive/Colab Notebooks/SP Cup 2025/Dataset/Tar Files/valid_fake.tar"
with tarfile.open(fake, 'r') as tar:
    tar.extractall(path=output_dir)
    print("valid real Files extracted successfully!")


tar.close()

print("finish")

train_dir = "/content/train"
valid_dir = "/content/valid"

train_real = len(os.listdir(f"{train_dir}/real"))
train_fake = len(os.listdir(f"{train_dir}/fake"))
valid_real = len(os.listdir(f"{valid_dir}/real"))
valid_fake = len(os.listdir(f"{valid_dir}/fake"))

print(f"Training dataset size: {train_real + train_fake} (Real: {train_real}, Fake: {train_fake})")
print(f"Validation dataset size: {valid_real + valid_fake} (Real: {valid_real}, Fake: {valid_fake})")

# Configure for maximum GPU utilization
torch.backends.cudnn.benchmark = True
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True

"""## Preparing and Loading the Dataset"""

# Hyperparameters
BATCH_SIZE = 256  # Increased batch size
NUM_WORKERS = 4
PREFETCH_FACTOR = 2
SAMPLES_PER_EPOCH = 200  # Number of batches per epoch as suggested by tutor
EPOCHS = 20
#LEARNING_RATE = 0.001  # Adjusted learning rate

# Data transforms
train_transform = transforms.Compose([
    transforms.RandomResizedCrop(128, scale=(0.8, 1.0)),
    transforms.RandomHorizontalFlip(),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Re-enable and adjust
    transforms.RandomRotation(15),  # Increased rotation
    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # Add translation
    transforms.RandomPerspective(distortion_scale=0.2),  # Add perspective changes
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.4598, 0.3929, 0.3792], std=[0.2327, 0.2046, 0.2103])
])

valid_transform = transforms.Compose([
    transforms.RandomResizedCrop(128, scale=(0.8, 1.0)),
    transforms.RandomHorizontalFlip(),
    #transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
    transforms.RandomRotation(10),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.3996, 0.3194, 0.3223], std=[0.2272, 0.1706, 0.1718])
])

"""## Model"""

class FrequencyBranch(nn.Module):
    def __init__(self, output_size=128, hidden_size1=512, hidden_size2=256):
        super(FrequencyBranch, self).__init__()
        input_size = 3 * 128 * 128 * 2
        self.fc1 = nn.Linear(input_size, hidden_size1)
        self.fc2 = nn.Linear(hidden_size1, hidden_size2)
        self.fc3 = nn.Linear(hidden_size2, output_size)
        self.relu = nn.ReLU()

    def forward(self, img):
        # GPU-optimized FFT operations
        f_transform = torch.fft.fft2(img)
        f_transform_shifted = torch.fft.fftshift(f_transform)
        amplitude = torch.abs(f_transform_shifted)
        phase = torch.angle(f_transform_shifted)
        features = torch.cat((amplitude.flatten(1), phase.flatten(1)), dim=1)

        x = self.relu(self.fc1(features))
        x = self.relu(self.fc2(x))
        return self.fc3(x)

class PreTrainedBranch(nn.Module):
    def __init__(self, input_channels=3, output_features=128):
        super(PreTrainedBranch, self).__init__()
        self.efficientnet = efficientnet_b6(weights=EfficientNet_B6_Weights.IMAGENET1K_V1)

        # Unfreeze more layers
        for param in self.efficientnet.parameters():
            param.requires_grad = True

        self.efficientnet.classifier = nn.Sequential(
            nn.Dropout(p=0.2, inplace=True),
            nn.Linear(self.efficientnet.classifier[1].in_features, output_features),
        )

    def forward(self, x):
        return self.efficientnet(x)

class CombinedModel(nn.Module):
    def __init__(self):
        super(CombinedModel, self).__init__()
        self.freq_branch = FrequencyBranch(output_size=128)
        self.conv_branch = PreTrainedBranch(output_features=128)
        self.fc1 = nn.Linear(256, 64)
        self.fc2 = nn.Linear(64, 32)
        self.fc3 = nn.Linear(32, 1)
        self.dropout = nn.Dropout(0.5)

    def forward(self, x):
        freq_output = self.freq_branch(x)
        conv_output = self.conv_branch(x)
        combined = torch.cat((freq_output, conv_output), dim=1)
        x = torch.relu(self.fc1(combined))
        x = self.dropout(x)
        x = torch.relu(self.fc2(x))
        x = self.dropout(x)
        return self.fc3(x)

"""## Dataloaders"""

def create_data_loaders():
    # Create datasets
    train_dataset = datasets.ImageFolder(train_dir, transform=train_transform)
    valid_dataset = datasets.ImageFolder(valid_dir, transform=valid_transform)

    # Create subset indices for training
    train_indices = torch.randperm(len(train_dataset))[:SAMPLES_PER_EPOCH * BATCH_SIZE]
    train_subset = Subset(train_dataset, train_indices)

    # Create data loaders with optimized settings
    train_loader = DataLoader(
        train_subset,
        batch_size=BATCH_SIZE,
        shuffle=True,
        num_workers=NUM_WORKERS,
        pin_memory=True,
        prefetch_factor=PREFETCH_FACTOR,
        persistent_workers=True
    )

    valid_loader = DataLoader(
        valid_dataset,
        batch_size=128, # different batch size for validation
        shuffle=False,
        num_workers=NUM_WORKERS,
        pin_memory=True
    )

    return train_loader, valid_loader

"""## Metrics"""

def calculate_metrics(y_true, y_pred):
    """
    Calculate precision, recall, and F1 score
    Args:
        y_true: Ground truth labels
        y_pred: Predicted labels
    Returns:
        Dictionary containing precision, recall, F1 score, and confusion matrix
    """
    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')
    conf_matrix = confusion_matrix(y_true, y_pred)

    return {
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'confusion_matrix': conf_matrix
    }

def plot_confusion_matrix(conf_matrix, save_path='confusion_matrix.png'):
    """
    Plot and save confusion matrix
    """
    plt.figure(figsize=(8, 6))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
                xticklabels=['Real', 'Fake'],
                yticklabels=['Real', 'Fake'])
    plt.title('Confusion Matrix')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.savefig(save_path)
    plt.close()

def evaluate_model(model, data_loader, device):
    """
    Evaluate model and calculate all metrics
    Args:
        model: Trained model
        data_loader: DataLoader containing validation/test data
        device: Device to run evaluation on
    Returns:
        Dictionary containing all metrics and predictions
    """
    model.eval()
    all_predictions = []
    all_targets = []

    with torch.no_grad():
        for data, target in data_loader:
            data, target = data.to(device), target.to(device)
            output = model(data).squeeze()
            predictions = (torch.sigmoid(output) >= 0.5).cpu().numpy()
            all_predictions.extend(predictions)
            all_targets.extend(target.cpu().numpy())

    # Convert to numpy arrays
    all_predictions = np.array(all_predictions)
    all_targets = np.array(all_targets)

    # Calculate metrics
    metrics = calculate_metrics(all_targets, all_predictions)

    # Plot confusion matrix
    plot_confusion_matrix(metrics['confusion_matrix'])

    return metrics

"""## Training"""

def train_model():
    print("Initializing model...")
    model = CombinedModel().to(device)

    print("Setting up optimizer and loss function...")
    optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,
                                                     mode='min',
                                                     patience=2,
                                                     factor=0.1)
    loss_fn = nn.BCEWithLogitsLoss()
    scaler = torch.amp.GradScaler('cuda')  # Updated to fix deprecation warning

    print("Creating data loaders...")
    try:
        train_loader, valid_loader = create_data_loaders()
        print(f"Train loader length: {len(train_loader)}")
        print(f"Number of training samples: {len(train_loader.dataset)}")
    except Exception as e:
        print(f"Error in data loader creation: {e}")
        return

    print("\nStarting training loop...")
    for epoch in range(EPOCHS):
        print(f"\nEpoch {epoch+1}/{EPOCHS}")
        model.train()
        train_loss = 0
        train_acc = 0

        print("Creating data loaders...")
        try:
            train_loader, valid_loader = create_data_loaders()
            print(f"Train loader length: {len(train_loader)}")
            print(f"Number of training samples: {len(train_loader.dataset)}")
        except Exception as e:
            print(f"Error in data loader creation: {e}")
            return

        # Wrap the training loop in a try-except block
        try:
            for batch_idx, (data, target) in enumerate(tqdm(train_loader)):
                try:
                    # Debug print for first batch
                    if batch_idx == 0:
                        print(f"\nBatch shape: {data.shape}")
                        print(f"Target shape: {target.shape}")

                    data, target = data.to(device), target.float().to(device)

                    # Mixed precision training
                    with autocast('cuda'):
                        output = model(data).squeeze()
                        loss = loss_fn(output, target)

                    # Gradient scaling and optimization
                    optimizer.zero_grad(set_to_none=True)
                    scaler.scale(loss).backward()
                    scaler.unscale_(optimizer)
                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                    scaler.step(optimizer)
                    scaler.update()

                    # Calculate accuracy
                    with torch.no_grad():
                        pred = torch.sigmoid(output) >= 0.5
                        train_acc += pred.eq(target.view_as(pred)).sum().item()
                        train_loss += loss.item()

                    # Print progress every 10 batches
                    if batch_idx % 10 == 0:
                        print(f"\nProcessed {batch_idx * len(data)}/{len(train_loader.dataset)} samples")
                        print(f"Current batch loss: {loss.item():.4f}")

                    # Clear cache periodically
                    if batch_idx % 10 == 0:
                        torch.cuda.empty_cache()

                except Exception as e:
                    print(f"Error in batch {batch_idx}: {e}")
                    continue

            # Calculate epoch statistics
            train_loss /= len(train_loader)
            train_acc = 100. * train_acc / len(train_loader.dataset)

            print(f'\nEpoch: {epoch+1}')
            print(f'Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.2f}%')

            # Validation
            model.eval()
            valid_loss = 0
            valid_acc = 0

            print("\nStarting validation...")
            with torch.no_grad():
                for data, target in valid_loader:
                    data, target = data.to(device), target.float().to(device)
                    output = model(data).squeeze()
                    valid_loss += loss_fn(output, target).item()
                    pred = torch.sigmoid(output) >= 0.5
                    valid_acc += pred.eq(target.view_as(pred)).sum().item()

            valid_loss /= len(valid_loader)
            valid_acc = 100. * valid_acc / len(valid_loader.dataset)

            print(f'Validation Loss: {valid_loss:.4f}, Validation Accuracy: {valid_acc:.2f}%\n')

            scheduler.step(valid_loss)

            # Save model checkpoint
            checkpoint_path = f'checkpoint_epoch_{epoch+1}.pt'
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'train_loss': train_loss,
                'valid_loss': valid_loss,
            }, checkpoint_path)
            print(f"Saved checkpoint to {checkpoint_path}")

        except Exception as e:
            print(f"Error in epoch {epoch+1}: {e}")
            continue

if __name__ == "__main__":
    try:
        # Print GPU information
        if torch.cuda.is_available():
            print(f"GPU: {torch.cuda.get_device_name(0)}")
            print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")

        # Print dataset information
        print(f"\nTraining directory: {train_dir}")
        print(f"Validation directory: {valid_dir}")

        train_model()
    except Exception as e:
        print(f"Fatal error: {e}")
        raise