{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, models\n",
    "from model import CombinedModel    \n",
    "from datasets import get_data_loaders\n",
    "from train import train\n",
    "#from test import evaluate, load_model\n",
    "\n",
    "# from tensorboard_utils import setup_tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(learning_rate=0.001, checkpoint_dir='./checkpoints', log_dir='./logs', train_dir='D:/Projects/SP CUP Dataset/valid', test_dir='D:/Projects/SP CUP Dataset/valid', batch_size=16, epochs=2)\n"
     ]
    }
   ],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='Training script')\n",
    "    parser.add_argument('--learning_rate', type=float, default=1e-3, help='Learning rate for the optimizer')\n",
    "    parser.add_argument('--checkpoint_dir', type=str, default='./checkpoints', help='Directory to save checkpoints')\n",
    "    parser.add_argument('--log_dir', type=str, default='./logs', help='Directory for TensorBoard logs')\n",
    "    parser.add_argument('--train_dir', type=str, default='D:/Projects/SP CUP Dataset/valid', help='Directory for training data')\n",
    "    parser.add_argument('--test_dir', type=str, default='D:/Projects/SP CUP Dataset/valid', help='Directory for testing data')\n",
    "    parser.add_argument('--batch_size', type=int, default=16, help='Batch size for training')\n",
    "    parser.add_argument('--epochs', type=int, default=2, help='Number of epochs to train')\n",
    "    \n",
    "    # Use parse_known_args to avoid errors\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "args = parse_args()\n",
    "\n",
    "os.makedirs(args.checkpoint_dir, exist_ok=True)\n",
    "os.makedirs(args.log_dir, exist_ok=True)\n",
    "\n",
    "print(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup TensorBoard\n",
    "# writer = setup_tensorboard(args.log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Model\n",
      "CombinedModel(\n",
      "  (freq_branch): FrequencyBranch(\n",
      "    (fc1): Linear(in_features=98304, out_features=512, bias=True)\n",
      "    (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (fc3): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (relu): ReLU()\n",
      "  )\n",
      "  (conv_branch): ConvBranch(\n",
      "    (model): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (4): ReLU()\n",
      "        (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (6): Dropout(p=0.3, inplace=False)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (4): ReLU()\n",
      "        (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (6): Dropout(p=0.3, inplace=False)\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (4): ReLU()\n",
      "        (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (6): Dropout(p=0.3, inplace=False)\n",
      "      )\n",
      "      (3): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (4): ReLU()\n",
      "        (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (6): Dropout(p=0.3, inplace=False)\n",
      "      )\n",
      "      (4): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "      (5): Flatten(start_dim=1, end_dim=-1)\n",
      "      (6): Linear(in_features=512, out_features=128, bias=True)\n",
      "      (7): ReLU()\n",
      "      (8): Dropout(p=0.5, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (fc1): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "print(\"Initializing Model\")\n",
    "model = CombinedModel()\n",
    "\n",
    "\"\"\"model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "num_features = model.fc.in_features  # Get the number of features from the current fc layer\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(num_features, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(512, 1), # Output layer for binary classification (Fake/Real)\n",
    "    nn.Sigmoid()\n",
    ")\"\"\"\n",
    "\n",
    "print(model)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"device: {device}\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.3996, 0.3194, 0.3223], [0.2321, 0.1766, 0.1816])\n",
    "])\n",
    "train_loader, valid_loader = get_data_loaders(args.train_dir, args.test_dir, args.batch_size, transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"Start Training\")\n",
    "train(model, train_loader, criterion, optimizer, args.epochs, args.checkpoint_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "\n",
    "# Modify the fully connected layer to match the saved model\n",
    "num_features = loaded_model.fc.in_features\n",
    "loaded_model.fc = nn.Sequential(\n",
    "\tnn.Linear(num_features, 512),\n",
    "\tnn.ReLU(),\n",
    "\tnn.Dropout(0.5),\n",
    "\tnn.Linear(512, 1),\n",
    "\tnn.Sigmoid()\n",
    ")\n",
    "\n",
    "MODEL_SAVE_PATH = args.checkpoint_dir + '/final_checkpoint.pth'\n",
    "state_dict = torch.load(MODEL_SAVE_PATH)\n",
    "\n",
    "loaded_model.load_state_dict(state_dict)\n",
    "loaded_model = loaded_model.to(device)\n",
    "print(f\"Model loaded from {MODEL_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set both models to evaluation mode\n",
    "model.eval()\n",
    "loaded_model.eval()\n",
    "\n",
    "# Compare the state dictionaries\n",
    "def compare_models(model1, model2):\n",
    "    model1_dict = model1.state_dict()\n",
    "    model2_dict = model2.state_dict()\n",
    "    \n",
    "    for key in model1_dict:\n",
    "        if not torch.equal(model1_dict[key], model2_dict[key]):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# Check if the parameters are identical\n",
    "are_identical = compare_models(model, loaded_model)\n",
    "print(f\"Are the trained model and loaded model parameters identical? {are_identical}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checkpoint_path = os.path.join(args.checkpoint_dir, 'final_checkpoint.pth')\n",
    "#print(f\"Loading model from {checkpoint_path}\")\n",
    "#model = load_model(checkpoint_path, model)\n",
    "\n",
    "evaluate(loaded_model, valid_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
