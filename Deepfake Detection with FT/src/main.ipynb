{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, models\n",
    "from models import Model\n",
    "from datasets import get_data_loaders\n",
    "from train import train\n",
    "from test import evaluate, load_model\n",
    "from pathlib import Path\n",
    "# from tensorboard_utils import setup_tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(learning_rate=0.001, checkpoint_dir='./checkpoints', log_dir='./logs', train_dir='D:/sp_cup/dataset/valid', test_dir='D:/sp_cup/dataset/valid', batch_size=16, epochs=20)\n"
     ]
    }
   ],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='Training script')\n",
    "    parser.add_argument('--learning_rate', type=float, default=1e-3, help='Learning rate for the optimizer')\n",
    "    parser.add_argument('--checkpoint_dir', type=str, default='./checkpoints', help='Directory to save checkpoints')\n",
    "    parser.add_argument('--log_dir', type=str, default='./logs', help='Directory for TensorBoard logs')\n",
    "    parser.add_argument('--train_dir', type=str, default='D:/sp_cup/dataset/valid', help='Directory for training data')\n",
    "    parser.add_argument('--test_dir', type=str, default='D:/sp_cup/dataset/valid', help='Directory for testing data')\n",
    "    parser.add_argument('--batch_size', type=int, default=16, help='Batch size for training')\n",
    "    parser.add_argument('--epochs', type=int, default=20, help='Number of epochs to train')\n",
    "    \n",
    "    # Use parse_known_args to avoid errors\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "args = parse_args()\n",
    "\n",
    "os.makedirs(args.checkpoint_dir, exist_ok=True)\n",
    "os.makedirs(args.log_dir, exist_ok=True)\n",
    "\n",
    "print(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup TensorBoard\n",
    "# writer = setup_tensorboard(args.log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "print(\"Initializing Model\")\n",
    "model = Model()\n",
    "\n",
    "\"\"\"model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "num_features = model.fc.in_features  # Get the number of features from the current fc layer\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(num_features, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(512, 1), # Output layer for binary classification (Fake/Real)\n",
    "    nn.Sigmoid()\n",
    ")\"\"\"\n",
    "\n",
    "print(model)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"device: {device}\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor()\n",
    "    #transforms.Normalize([0.3996, 0.3194, 0.3223], [0.2321, 0.1766, 0.1816])\n",
    "])\n",
    "train_loader, valid_loader = get_data_loaders(args.train_dir, args.test_dir, args.batch_size, transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"Start Training\")\n",
    "train(model, train_loader, criterion, optimizer, args.epochs, args.checkpoint_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from models/model_01_resnet50.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VICTUS\\AppData\\Local\\Temp\\ipykernel_10332\\1059343257.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(MODEL_SAVE_PATH)\n"
     ]
    }
   ],
   "source": [
    "loaded_model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "\n",
    "# Modify the fully connected layer to match the saved model\n",
    "num_features = loaded_model.fc.in_features\n",
    "loaded_model.fc = nn.Sequential(\n",
    "\tnn.Linear(num_features, 512),\n",
    "\tnn.ReLU(),\n",
    "\tnn.Dropout(0.5),\n",
    "\tnn.Linear(512, 1),\n",
    "\tnn.Sigmoid()\n",
    ")\n",
    "\n",
    "MODEL_SAVE_PATH = 'models/model_01_resnet50.pth'\n",
    "state_dict = torch.load(MODEL_SAVE_PATH)\n",
    "\n",
    "loaded_model.load_state_dict(state_dict)\n",
    "loaded_model = loaded_model.to(device)\n",
    "print(f\"Model loaded from {MODEL_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are the trained model and loaded model parameters identical? True\n"
     ]
    }
   ],
   "source": [
    "# Set both models to evaluation mode\n",
    "model.eval()\n",
    "loaded_model.eval()\n",
    "\n",
    "# Compare the state dictionaries\n",
    "def compare_models(model1, model2):\n",
    "    model1_dict = model1.state_dict()\n",
    "    model2_dict = model2.state_dict()\n",
    "    \n",
    "    for key in model1_dict:\n",
    "        if not torch.equal(model1_dict[key], model2_dict[key]):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# Check if the parameters are identical\n",
    "are_identical = compare_models(model, loaded_model)\n",
    "print(f\"Are the trained model and loaded model parameters identical? {are_identical}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the validation images: 49.61%\n",
      "True Negatives (Real identified as Real): 1524\n",
      "False Positives (Real identified as Fake): 0\n",
      "False Negatives (Fake identified as Real): 1548\n",
      "True Positives (Fake identified as Fake): 0\n",
      "Confusion Matrix:\n",
      "[[1524    0]\n",
      " [1548    0]]\n"
     ]
    }
   ],
   "source": [
    "#checkpoint_path = os.path.join(args.checkpoint_dir, 'final_checkpoint.pth')\n",
    "#print(f\"Loading model from {checkpoint_path}\")\n",
    "#model = load_model(checkpoint_path, model)\n",
    "\n",
    "evaluate(loaded_model, valid_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
