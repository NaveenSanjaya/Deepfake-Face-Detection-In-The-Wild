{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, models\n",
    "from model import CombinedModel    \n",
    "from datasets import get_data_loaders, compute_mean_and_std\n",
    "from train import train\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(learning_rate=0.001, checkpoint_dir='./checkpoints', log_dir='./logs', train_dir='D:/sp_cup/dataset/train', test_dir='D:/sp_cup/dataset//valid', batch_size=16, epochs=2)\n"
     ]
    }
   ],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='Training script test')\n",
    "    parser.add_argument('--learning_rate', type=float, default=1e-3, help='Learning rate for the optimizer')\n",
    "    parser.add_argument('--checkpoint_dir', type=str, default='./checkpoints', help='Directory to save checkpoints')\n",
    "    parser.add_argument('--log_dir', type=str, default='./logs', help='Directory for TensorBoard logs')\n",
    "    parser.add_argument('--train_dir', type=str, default='D:/sp_cup/dataset/train', help='Directory for training data')\n",
    "    parser.add_argument('--test_dir', type=str, default='D:/sp_cup/dataset//valid', help='Directory for testing data')\n",
    "    parser.add_argument('--batch_size', type=int, default=16, help='Batch size for training')\n",
    "    parser.add_argument('--epochs', type=int, default=2, help='Number of epochs to train')\n",
    "    \n",
    "    # Use parse_known_args to avoid errors\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "args = parse_args()\n",
    "\n",
    "os.makedirs(args.checkpoint_dir, exist_ok=True)\n",
    "os.makedirs(args.log_dir, exist_ok=True)\n",
    "\n",
    "print(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrequencyBranch(nn.Module):\n",
    "    def __init__(self, output_size=128, hidden_size1=512, hidden_size2=256):\n",
    "        super(FrequencyBranch, self).__init__()\n",
    "        # Recalculate input size for single sample (without batch dimension)\n",
    "        input_size = 3 * 128 * 128 * 2  # 128x128 pixels, 2 features (amplitude and phase)\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, img):\n",
    "        # img shape: [batch_size, channels, height, width]\n",
    "        batch_size = img.shape[0]\n",
    "        \n",
    "        # Move to CPU for numpy operations\n",
    "        img_np = img.cpu().numpy()\n",
    "        \n",
    "        # Process each image in the batch\n",
    "        batch_features = []\n",
    "        for i in range(batch_size):\n",
    "            # Apply FFT to single image\n",
    "            f_transform = np.fft.fft2(img_np[i])\n",
    "            f_transform_shifted = np.fft.fftshift(f_transform)\n",
    "            amplitude = np.abs(f_transform_shifted)\n",
    "            phase = np.angle(f_transform_shifted)\n",
    "            \n",
    "            # Concatenate amplitude and phase\n",
    "            features = np.concatenate((amplitude.flatten(), phase.flatten()))\n",
    "            batch_features.append(features)\n",
    "            \n",
    "        # Stack all features into a batch\n",
    "        batch_features = np.stack(batch_features)\n",
    "        \n",
    "        # Convert back to tensor\n",
    "        input_tensor = torch.tensor(batch_features, dtype=torch.float32, device=img.device)\n",
    "        # Pass through network\n",
    "        x = self.relu(self.fc1(input_tensor))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        output_vector = self.fc3(x)\n",
    "        \n",
    "        # output_vector shape will be [batch_size, output_size]\n",
    "        return output_vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBranch(nn.Module):\n",
    "    def __init__(self, input_channels=3, output_features=128):\n",
    "        super(ConvBranch, self).__init__()\n",
    "        def conv_block(in_channels, out_channels):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                nn.Dropout(0.3)\n",
    "            )\n",
    "        self.model = nn.Sequential(\n",
    "            conv_block(input_channels, 64),\n",
    "            conv_block(64, 128),\n",
    "            conv_block(128, 256),\n",
    "            conv_block(256, 512),\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512, output_features),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "\n",
    "    def forward(self, image):\n",
    "        return self.model(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.freq_branch = FrequencyBranch(output_size=128)\n",
    "        self.conv_branch = ConvBranch(output_features=128)\n",
    "        self.fc1 = nn.Linear(256, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Same input for both branches\n",
    "        freq_output = self.freq_branch(x)\n",
    "        conv_output = self.conv_branch(x)\n",
    "\n",
    "        combined = torch.cat((freq_output, conv_output), dim=1)\n",
    "        x = torch.relu(self.fc1(combined))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_fft(image):\n",
    "    \"\"\"Plot the original image, magnitude spectrum and phase spectrum\"\"\"\n",
    "\n",
    "    f_transform = np.fft.fft2(image)\n",
    "    f_transform_shifted = np.fft.fftshift(f_transform)\n",
    "    magnitude = np.abs(f_transform_shifted)\n",
    "    phase = np.angle(f_transform_shifted)\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    ax[0].imshow(image, cmap='gray')\n",
    "    ax[0].set_title('Original Image')\n",
    "    ax[0].axis('off')\n",
    "    \n",
    "    ax[1].imshow(np.log1p(magnitude), cmap='gray')\n",
    "    ax[1].set_title('Magnitude Spectrum')\n",
    "    ax[1].axis('off')\n",
    "    \n",
    "    ax[2].imshow(phase, cmap='gray')\n",
    "    ax[2].set_title('Phase Spectrum')\n",
    "    ax[2].axis('off')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "from imageio import imread\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "from PIL import Image\n",
    "from skimage.io import imread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(TorchDataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = imread(self.image_paths[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = Image.fromarray(image)\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_paths_and_labels(data_dir):\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    class_names = os.listdir(data_dir)\n",
    "    print(f\"Class names: {class_names}\")    \n",
    "    class_to_idx = {class_name: idx for idx, class_name in enumerate(class_names)}\n",
    "    print(f\"Class to index: {class_to_idx}\")\n",
    "    for class_name in class_names:\n",
    "        class_dir = os.path.join(data_dir, class_name)\n",
    "        for img_name in os.listdir(class_dir):\n",
    "            image_paths.append(os.path.join(class_dir, img_name))\n",
    "            labels.append(class_to_idx[class_name])\n",
    "    return image_paths, labels\n",
    "\n",
    "\n",
    "def get_data_loaders(train_dir, test_dir, batch_size, transform=None):\n",
    "    train_image_paths, train_labels = get_image_paths_and_labels(train_dir)\n",
    "    test_image_paths, test_labels = get_image_paths_and_labels(test_dir)\n",
    "\n",
    "    train_dataset = Dataset(train_image_paths, train_labels, transform)\n",
    "    test_dataset = Dataset(test_image_paths, test_labels, transform)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "# Compute mean and std of the dataset\n",
    "def compute_mean_and_std(data_dir):\n",
    "    \"\"\"\n",
    "    Compute per-channel mean and std of the dataset (to be used in transforms.Normalize())\n",
    "    \"\"\"\n",
    "\n",
    "    cache_file = \"mean_and_std.pt\"\n",
    "    if os.path.exists(cache_file):\n",
    "        print(f\"Reusing cached mean and std\")\n",
    "        d = torch.load(cache_file)\n",
    "\n",
    "        return d[\"mean\"], d[\"std\"]\n",
    "\n",
    "    ds = datasets.ImageFolder(\n",
    "        data_dir, transform=transforms.Compose([transforms.ToTensor()])\n",
    "    )\n",
    "    dl = torch.utils.data.DataLoader(\n",
    "        ds, batch_size=1, num_workers=multiprocessing.cpu_count()\n",
    "    )\n",
    "\n",
    "    mean = 0.0\n",
    "    for images, _ in tqdm(dl, total=len(ds), desc=\"Computing mean\", ncols=80):\n",
    "        batch_samples = images.size(0)\n",
    "        images = images.view(batch_samples, images.size(1), -1)\n",
    "        mean += images.mean(2).sum(0)\n",
    "    mean = mean / len(dl.dataset)\n",
    "\n",
    "    var = 0.0\n",
    "    npix = 0\n",
    "    for images, _ in tqdm(dl, total=len(ds), desc=\"Computing std\", ncols=80):\n",
    "        batch_samples = images.size(0)\n",
    "        images = images.view(batch_samples, images.size(1), -1)\n",
    "        var += ((images - mean.unsqueeze(1)) ** 2).sum([0, 2])\n",
    "        npix += images.nelement()\n",
    "\n",
    "    std = torch.sqrt(var / (npix / 3))\n",
    "\n",
    "    # Cache results so we don't need to redo the computation\n",
    "    torch.save({\"mean\": mean, \"std\": std}, cache_file)\n",
    "\n",
    "    return mean, std\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Model\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing Model\")\n",
    "model = CombinedModel()\n",
    "\n",
    "\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"device: {device}\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "\n",
    "# print(criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing mean: 100%|██████████████████| 262160/262160 [05:08<00:00, 848.59it/s]\n",
      "Computing std: 100%|███████████████████| 262160/262160 [05:28<00:00, 798.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: tensor([0.4578, 0.3924, 0.3772]), Std: tensor([0.2650, 0.2399, 0.2503])\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "dataset_mean, dataset_std = compute_mean_and_std(args.train_dir)\n",
    "print(f\"Mean: {dataset_mean}, Std: {dataset_std}\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.3996, 0.3194, 0.3223], [0.2321, 0.1766, 0.1816])\n",
    "])\n",
    "train_loader, valid_loader = get_data_loaders(args.train_dir, args.test_dir, args.batch_size, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4577801823616028, 0.3923501670360565, 0.37724995613098145]\n",
      "[0.2649841904640198, 0.23986703157424927, 0.2503381669521332]\n"
     ]
    }
   ],
   "source": [
    "print(torch.Tensor.tolist(dataset_mean))\n",
    "print(torch.Tensor.tolist(dataset_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(torch.Tensor.tolist(dataset_mean), torch.Tensor.tolist(dataset_std))\n",
    "])\n",
    "train_loader, valid_loader = get_data_loaders(args.train_dir, args.test_dir, args.batch_size, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Get a random image and its label from the dataset\n",
    "dataset = train_loader.dataset\n",
    "random_idx = random.randint(0, len(dataset) - 1)\n",
    "image_path = dataset.image_paths[random_idx]\n",
    "label = dataset.labels[random_idx]\n",
    "\n",
    "# Load the original image\n",
    "original_image = Image.open(image_path)\n",
    "\n",
    "# Apply the transformation\n",
    "transformed_image = transform(original_image)\n",
    "\n",
    "# Convert the transformed image to numpy array for plotting\n",
    "transformed_image_np = transformed_image.permute(1, 2, 0).numpy()\n",
    "\n",
    "# Plot the original and transformed images\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "ax[0].imshow(original_image)\n",
    "ax[0].set_title('Original Image')\n",
    "ax[0].axis('off')\n",
    "\n",
    "ax[1].imshow(transformed_image_np)\n",
    "ax[1].set_title('Transformed Image')\n",
    "ax[1].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
